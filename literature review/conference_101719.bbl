% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{zhong2023ptm}
Q.~Zhong, X.~Xiao, Y.~Qiu, Z.~Xu, C.~Chen, B.~Chong, X.~Zhao, S.~Hai, S.~Li,
  Z.~An, and L.~Dai, ``Protein posttranslational modifications in health and
  diseases: Functions, regulatory mechanisms, and therapeutic implications,''
  \emph{MedComm}, vol.~4, no.~3, p. e261, 2023.

\bibitem{cheng2023research}
X.~Cheng, K.~Wang, Y.~Zhao, and K.~Wang, ``Research progress on
  post-translational modification of proteins and cardiovascular diseases,''
  \emph{Cell Death Discovery}, vol.~9, no.~1, p. 275, 2023.

\bibitem{messner2023mass}
C.~B. Messner, V.~Demichev, Z.~Wang, J.~Hartl, G.~Kustatscher, M.~M{\"u}lleder,
  and M.~Ralser, ``Mass spectrometry-based high-throughput proteomics and its
  role in biomedical studies and systems biology,'' \emph{Proteomics}, vol.~23,
  no. 7-8, p. 2200013, 2023.

\bibitem{li2022dbptm}
Z.~Li, S.~Li, M.~Luo, J.-H. Jhong, W.~Li, L.~Yao, Y.~Pang, Z.~Wang, R.~Wang,
  R.~Ma \emph{et~al.}, ``dbptm in 2022: an updated database for exploring
  regulatory networks and functional associations of protein post-translational
  modifications,'' \emph{Nucleic acids research}, vol.~50, no.~D1, pp.
  D471--D479, 2022.

\bibitem{harrison2003method}
P.~M. Harrison and M.~Gerstein, ``A method to assess compositional bias in
  biological sequences and its application to prion-like
  glutamine/asparagine-rich domains in eukaryotic proteomes,'' \emph{Genome
  biology}, vol.~4, no.~6, p. R40, 2003.

\bibitem{harrison2017flps}
P.~M. Harrison, ``flps: Fast discovery of compositional biases for the protein
  universe,'' \emph{Bmc Bioinformatics}, vol.~18, no.~1, p. 476, 2017.

\bibitem{shrestha2024post}
P.~Shrestha, J.~Kandel, H.~Tayara, and K.~T. Chong, ``Post-translational
  modification prediction via prompt-based fine-tuning of a gpt-2 model,''
  \emph{Nature Communications}, vol.~15, no.~1, p. 6699, 2024.

\bibitem{wang2017musitedeep}
D.~Wang, S.~Zeng, C.~Xu, W.~Qiu, Y.~Liang, and T.~Joshi, ``Musitedeep: a
  deep-learning framework for general and kinase-specific phosphorylation site
  prediction,'' \emph{Bioinformatics}, vol.~33, no.~24, pp. 3909--3916, 2017.

\bibitem{fu2019deepubi}
H.~Fu, Y.~Yang, X.~Wang, H.~Wang, and Y.~Xu, ``Deepubi: a deep learning
  framework for prediction of ubiquitination sites in proteins,'' \emph{BMC
  Bioinformatics}, vol.~20, no.~1, 2019.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in
  \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem{elnaggar2020}
A.~Elnaggar \emph{et~al.}, ``Protbert: A pretrained deep learning model for
  protein sequence representation,'' \emph{Bioinformatics}, vol.~36, no.~7, pp.
  2099--2108, 2020.

\bibitem{elnaggar2021prottranscrackinglanguagelifes}
\BIBentryALTinterwordspacing
A.~Elnaggar, M.~Heinzinger, C.~Dallago, G.~Rihawi, Y.~Wang, L.~Jones, T.~Gibbs,
  T.~Feher, C.~Angerer, M.~Steinegger, D.~Bhowmik, and B.~Rost, ``Prottrans:
  Towards cracking the language of life's code through self-supervised deep
  learning and high performance computing,'' 2021. [Online]. Available:
  \url{https://arxiv.org/abs/2007.06225}
\BIBentrySTDinterwordspacing

\bibitem{lin2022}
Z.~Lin \emph{et~al.}, ``Esm-2: Protein sequence embeddings for downstream
  analysis,'' \emph{Nature Communications}, vol.~13, pp. 1--9, 2022.

\bibitem{rives2021scaling}
A.~Rives, J.~Meier, T.~Sercu, S.~Goyal, Z.~Lin, J.~Liu, D.~Guo, M.~Ott, C.~L.
  Zitnick, J.~Ma, and R.~Fergus, ``Biological structure and function emerge
  from scaling unsupervised learning to 250 million protein sequences,''
  \emph{Proceedings of the National Academy of Sciences}, vol. 118, no.~15,
  2021.

\bibitem{elnaggar2023}
A.~Elnaggar \emph{et~al.}, ``Ankh: A transformer-based protein language model
  for molecular interaction prediction,'' \emph{Cell}, vol. 184, pp.
  3712--3725, 2023.

\bibitem{alley2019unirep}
E.~C. Alley, G.~Khimulya, S.~S. Biswas, M.~AlQuraishi, and G.~M. Church,
  ``Unified rational protein engineering with sequence-based deep
  representation learning,'' \emph{Nature Methods}, vol.~16, no.~12, pp.
  1315--1322, 2019.

\bibitem{dettmers2023qloraefficientfinetuningquantized}
\BIBentryALTinterwordspacing
T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer, ``Qlora: Efficient
  finetuning of quantized llms,'' 2023. [Online]. Available:
  \url{https://arxiv.org/abs/2305.14314}
\BIBentrySTDinterwordspacing

\bibitem{madani2020progen}
A.~Madani, B.~McCann, N.~Naik, N.~S. Keskar, N.~Anand, R.~R. Eguchi, P.-S.
  Huang, and R.~Socher, ``Progen: Language modeling for protein generation,''
  \emph{arXiv preprint arXiv:2004.03497}, 2020.

\bibitem{ferruz2022protgpt2}
\BIBentryALTinterwordspacing
N.~Ferruz, S.~Schmidt, and B.~HÃ¶cker, ``Protgpt2 is a deep unsupervised
  language model for protein design,'' \emph{Nature Communications}, vol.~13,
  p. 4348, 2022. [Online]. Available:
  \url{https://doi.org/10.1038/s41467-022-32007-7}
\BIBentrySTDinterwordspacing

\bibitem{pratyush2024lmcrot}
P.~Pratyush, S.~Bahmani, S.~Pokharel, H.~D. Ismail, and D.~B. Kc, ``Lmcrot: an
  enhanced protein crotonylation site predictor by leveraging an interpretable
  window-level embedding from a transformer-based protein language model,''
  \emph{Bioinformatics}, vol.~40, no.~5, p. btae290, 2024.

\bibitem{zaheer2020bigbird}
M.~Zaheer, G.~Guruganesh, A.~Dubey, J.~Ainslie, C.~Alberti, S.~Onta\~{n}\'{o}n,
  P.~Pham, A.~Ravula, Q.~Wang, L.~Yang, and A.~Ahmed, ``Big bird: Transformers
  for longer sequences,'' in \emph{Advances in Neural Information Processing
  Systems}, 2020.

\bibitem{jumper2021highly}
J.~Jumper, R.~Evans, A.~Pritzel, T.~Green, M.~Figurnov, O.~Ronneberger,
  K.~Tunyasuvunakool, R.~Bates, A.~{\v{Z}}{\'\i}dek, A.~Potapenko
  \emph{et~al.}, ``Highly accurate protein structure prediction with
  alphafold,'' \emph{nature}, vol. 596, no. 7873, pp. 583--589, 2021.

\bibitem{gu2020hippo}
A.~Gu, K.~Goel, and C.~R{\'e}, ``Hippo: Recurrent memory with optimal
  polynomial projections,'' in \emph{Advances in Neural Information Processing
  Systems}, 2020.

\bibitem{gu2022s4}
A.~Gu and T.~Dao, ``Efficiently modeling long sequences with structured state
  spaces,'' in \emph{International Conference on Learning Representations},
  2022.

\bibitem{peng2024ptm}
Z.~Peng, ``Ptm-mamba: a ptm-aware protein language model with bidirectional
  gated mamba blocks,'' in \emph{Proceedings of the 33rd ACM International
  Conference on Information and Knowledge Management}, 2024, pp. 5475--5478.

\end{thebibliography}
